{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "drawn-violence",
   "metadata": {},
   "source": [
    "# StackRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "considered-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# 添加下面几行，否则LSTM有可能出bug\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hearing-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test)=keras.datasets.imdb.load_data(num_words=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "internal-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=10000\n",
    "embedding_dim=32\n",
    "word_num=500\n",
    "state_dim=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hollywood-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train=sequence.pad_sequences(x_train,maxlen=word_num)\n",
    "x_test=sequence.pad_sequences(x_test,maxlen=word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "formed-accessory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 500, 32)           8320      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 500, 32)           8320      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 344,993\n",
      "Trainable params: 344,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(Embedding(vocabulary,embedding_dim,input_length=word_num))\n",
    "model.add(LSTM(state_dim,return_sequences=True))\n",
    "model.add(LSTM(state_dim,return_sequences=True))\n",
    "model.add(LSTM(state_dim,return_sequences=False))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "weird-classic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 30s 134ms/step - loss: 0.5450 - acc: 0.6853 - val_loss: 0.3065 - val_acc: 0.8705\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 26s 130ms/step - loss: 0.2248 - acc: 0.9193 - val_loss: 0.2997 - val_acc: 0.8744\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 25s 130ms/step - loss: 0.1590 - acc: 0.9451 - val_loss: 0.3274 - val_acc: 0.8686\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 26s 131ms/step - loss: 0.1190 - acc: 0.9604 - val_loss: 0.3941 - val_acc: 0.8638\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 26s 131ms/step - loss: 0.0941 - acc: 0.9706 - val_loss: 0.4416 - val_acc: 0.8527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb513a18610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=1,epochs=5,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nasty-awareness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 19s 24ms/step - loss: 0.4416 - acc: 0.8527\n",
      "Loss:0.44, Accuracy:0.85\n"
     ]
    }
   ],
   "source": [
    "loss,acc=model.evaluate(x_test,y_test,verbose=1)\n",
    "print('Loss:%.2f, Accuracy:%.2f'%(loss,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-identification",
   "metadata": {},
   "source": [
    "# BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "satisfactory-sentence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzh/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 336,705\n",
      "Trainable params: 336,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "196/196 [==============================] - 19s 89ms/step - loss: 0.6302 - acc: 0.6213 - val_loss: 0.4301 - val_acc: 0.8189\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 17s 87ms/step - loss: 0.3222 - acc: 0.8724 - val_loss: 0.3275 - val_acc: 0.8603\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 17s 87ms/step - loss: 0.2197 - acc: 0.9198 - val_loss: 0.3054 - val_acc: 0.8774\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 17s 87ms/step - loss: 0.1704 - acc: 0.9414 - val_loss: 0.3470 - val_acc: 0.8643\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 17s 89ms/step - loss: 0.1313 - acc: 0.9574 - val_loss: 0.3457 - val_acc: 0.8659\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3457 - acc: 0.8659\n",
      "Loss:0.35, Accuracy:0.87\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# 添加下面几行，否则LSTM有可能出bug\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=keras.datasets.imdb.load_data(num_words=vocabulary)\n",
    "\n",
    "vocabulary=10000\n",
    "embedding_dim=32\n",
    "word_num=500\n",
    "state_dim=32\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "x_train=sequence.pad_sequences(x_train,maxlen=word_num)\n",
    "x_test=sequence.pad_sequences(x_test,maxlen=word_num)\n",
    "\n",
    "from keras.layers import Dense,Bidirectional\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(Embedding(vocabulary,embedding_dim,input_length=word_num))\n",
    "model.add(Bidirectional(LSTM(state_dim,return_sequences=False)))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=1,epochs=5,batch_size=128)\n",
    "\n",
    "loss,acc=model.evaluate(x_test,y_test,verbose=1)\n",
    "print('Loss:%.2f, Accuracy:%.2f'%(loss,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-classics",
   "metadata": {},
   "source": [
    "# Pretrain\n",
    "改进RNN->LSTM->StackLSTM->Bidirectional LSTM并没有特别大的提升，是因为Embedding的向量参数太多就存在overfitting  \n",
    "只改变RNN无济于事，通过Pretrain，借助更大的语料数据集可以更好地解决这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-heater",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
