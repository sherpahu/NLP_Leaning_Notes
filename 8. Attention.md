# 步骤

1. 计算weight，$ h_i $与$ s_j $计算$ \alpha_i $

   ![image-20210331210121028](8. Attention.assets/image-20210331210121028.png)

2. 通过$ \alpha_i $计算$ c_j $进而将Encoder的所有输出导入Decoder中

   ![image-20210331210019646](8. Attention.assets/image-20210331210019646.png)

3. 用Attention改造SimpleRNN

   ![image-20210331210535975](8. Attention.assets/image-20210331210535975.png)

4. 依次迭代

   

   

# 优势

1. 将Encoder的所有状态考虑进去了，不只是跟Simple RNN一样只考虑最终状态
2. 有侧重，通过$ \alpha_i $ 将权重考虑进去了

# 缺点

计算量大，Encoder有m层，Decoder有t层，则复杂度为O(mt)

![image-20210331210715526](8. Attention.assets/image-20210331210715526.png)